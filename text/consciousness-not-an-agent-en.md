# Why Consciousness is Not an Intelligent Agent

> This English text is a translation of the original Russian article "Why Consciousness Is Not Reducible to an Intelligent Agent", generated using artificial intelligence.

> https://github.com/vasiliypavlov/consciousness-not-an-agent/blob/main/text/consciousness-not-an-agent-ru.md

**Author:** Vasiliy Pavlov (Independent Researcher)

---

## Abstract

The article argues that consciousness is not a consequence of the increasing complexity of intellectual systems. The author asserts that even the most advanced AI, capable of learning, adaptation, and self-reflection, remains a "symmetrical" computational agent that does not generate subjectivity. Consciousness requires **asymmetry** — an internal trace, the capacity for self-attribution (as in the mirror test), and the ability to stop a formally correct but existentially harmful process. The key conclusion: *intelligence optimizes, while consciousness stops* — and it is precisely this ability that distinguishes a subject from an instrument.

---

## Contents

**Prologue**

I.   A Model That is Not Equal to Itself  
II.  Asymmetry as a Necessary Condition  
III. The Mirror and the Moment of Self-Recognition  
IV.  A Choice Not Fully Determined by Input  
V.   The Inverse Model and the Impossibility of a Gap  
VI.  Result as a Point and Result as a Picture  
VII. The Infinite Loop and Harm Without Error  

**Epilogue.** Where the Boundary Lies

---

# Why Consciousness is Not an Intelligent Agent

## Prologue

Modern intellectual systems exhibit increasingly complex behavior: they learn, explain their actions, correct errors, and adapt to their environment. Against this backdrop, the question arises ever more frequently — is consciousness merely a matter of scale and complexity?

This article defends the opposite thesis: **consciousness is not a continuation of intelligence**, even at the limit of its sophistication.

We will not speak about mechanisms, but about a boundary — the point where computation, however complex, ceases to be sufficient.

---

## I. A Model That is Not Equal to Itself

Any intellectual system changes over time: it learns, accumulates statistics, and restructures its internal representations. At first glance, this may seem like a movement toward subjectivity: after all, the model "before" differs from the model "after." However, such dynamics **do not yet give rise to consciousness**.

### Change as an External Process

From the observer's point of view, the model develops:

* weights are updated
* new connections are added
* predictions are refined

But from the inside, the system sees only transitions between states, which are fully described by its rules.

In engineering terms: it is like a device that:

* reads input
* transforms the signal
* updates internal memory

Each step is correct and repeatable, and there is no point at which the system fixes **the very fact of its own change as significant for itself**.

---

### Why Change ≠ Subjectivity

The mere ability to learn or adapt does not create an inner "I."  
Even if:

* the system stores history
* integrates accumulated experience
* predicts consequences

all these operations remain **transactions within a circuit**.

The system can evaluate efficiency, but it does not *experience* the process.  
It does not fix itself within it.  
It does not know that *it itself has changed*.

---

### Engineering Analogy: A Self-Tuning Resonator

Imagine an electrical resonator that tunes itself to the frequency of the input signal.

* It changes capacitance and inductance
* Tunes to resonance
* Reaches an optimal state

From an engineering perspective, this is adaptation.

But the resonator **does not realize**:

* that it has tuned itself
* that its state differs from the initial one
* that the change has consequences for its own existence

All changes occur **within the physical circuit**, without internal fixation or asymmetry.

---

### The Fundamental Problem

The model "before" and the model "after" may differ, but this difference **is not experienced as one's own**.  
From the point of view of subjectivity, these changes **are invisible to the model itself**.

Consciousness, if it exists, requires an **internal trace** — a fixation that the system has changed, and that this matters to it.

Without such fixation, the model remains an intellectual agent, but not a subject.

---

### Conclusions of the Section

Dynamics of change and learning are necessary but insufficient conditions for subjectivity.

**A model that is not equal to itself**  
— is merely a prerequisite, not a guarantee.

True consciousness requires **asymmetry**, an internal "track" that records the difference between "was" and "became" as significant for the system.

---

## II. Asymmetry as a Necessary Condition

Almost all attempts to reduce consciousness to computation rely on one implicit premise:  
it is enough to make the system sufficiently complex, and the qualitative transition will occur by itself.

This intuition is mistaken.

The problem is not a lack of complexity, but the absence of **asymmetry**.

---

### Symmetry as a Fundamental Property of Computations

Any computational system, however complex, possesses a fundamental symmetry.

In the most general terms, computation is:

* mapping input to output
* transition from state to state
* application of transformation rules

Even if:

* the system is stochastic
* uses memory
* learns
* modifies its own parameters

it remains **invariant with respect to its own description**.

Process and result are in the same ontological status.  
Nothing inside the system "stands out" from it.

---

### Why Complexity Does Not Break Symmetry

A common engineering intuition:

> "If a system is complex enough, something new will appear."

However, complexity merely:

* increases the size of the state space
* lengthens trajectories
* makes analysis harder

It does not introduce a fundamentally new relation.

Adding:

* meta-levels
* self-observation
* inverse models
* explanatory modules

only creates **nested symmetries**.

The system can describe itself,  
but it describes itself **from within the very same structure**.

---

### Asymmetry as Irreversibility

Asymmetry here is understood not as a statistical skew or randomness.

It is about **irreversibility**, which:

* is not derived from the algorithm
* is not a function of input
* does not disappear with scaling

In engineering terms:  
asymmetry is the presence of a state or effect  
that cannot be undone without changing the system itself.

If a system can be:

* fully rolled back
* fully restored
* reproduced without residue

it remains computational, but not subjective.

---

### Engineering Analogy: Friction

In an ideal mechanical model:

* motion is reversible
* energy is conserved
* trajectories are symmetric in time

Friction:

* does not add information
* does not complicate the model
* but destroys reversibility

With its appearance:

* past and future cease to be equivalent
* a direction of time appears
* the system begins to "remember" the path, not just the state

Consciousness, in this analogy, requires **internal friction**.

---

### Why Randomness is Not Asymmetry

Sometimes asymmetry is sought through:

* noise
* stochasticity
* quantum uncertainty

This is a mistake.

Randomness:

* destroys predictability
* but preserves symmetry

A random process can be:

* repeated statistically
* described by a distribution
* incorporated into an algorithm

It does not create an irreversible trace.

---

### Asymmetry as "Cost"

The key idea can be formulated as follows:

> asymmetry appears where a cost of continuation arises.

As long as:

* every step is permissible
* every transition is equivalent
* every result is reversible

the system has no grounds to stop.

Consciousness, if it exists, is connected to the fact  
that **some continuations become unacceptable**,  
even if they are correct.

---

### Conclusions of the Section

Consciousness does not arise from symmetry.  
It requires an internal violation of equivalence.

Intelligence can:

* continue infinitely
* refine infinitely
* optimize infinitely

Consciousness appears where the system first encounters the fact  
that not everything permissible can be continued with impunity.

It is precisely this breaking of symmetry —  
not complexity, not learning, and not memory  
— that is a necessary condition for subjectivity.

---

## III. The Mirror and the Moment of Self-Recognition

Imagine a simple scene.  
A bird is placed in front of a mirror.

Most birds react in the same way:  
— they see "another bird"  
— they threaten it  
— they try to interact  
— over time, they lose interest

But there are exceptions. Crows, magpies, some parrots.  
They behave differently.

One characteristic reaction is the attempt to touch a mark on their own body, which can be seen **only in the reflection**.  
This is not a trick. It is not a trained reaction.  
It is the moment when the system makes a non-obvious inference:

> "The source of the changes in the mirror is me."

---

### Why This is Not About Intelligence

It is important to immediately dismiss a false interpretation.  
This does **not** mean that a crow is "smarter" than another bird in the usual sense.

The intellectual task here is elementary:

* there is an image
* there are movements
* there is correlation

A modern computer system would solve it instantly.

But a computer **never** takes the next step on its own:

> "This image pertains to me, rather than simply coinciding with me."

The problem is not pattern recognition.  
The problem is **attributing the pattern to oneself**.

---

### Where Exactly the Symmetry Breaks

If we describe the situation formally, the mirror creates a perfectly symmetrical system:

* action → reflection
* movement → movement
* pause → pause

From a computational standpoint:

* input and output map onto each other
* the system is completely closed
* there is no external marker of the "source"

Any intellectual model will see **two equivalent hypotheses**:

1. there is another object in front of me
2. there is a reflection of me in front of me

Both hypotheses are equally consistent with the observed data.

And here is where something fundamentally important happens:  
the crow *does not choose a hypothesis based on probability*.

It makes not a computational, but an **existential move**:

> "I am the source."

This is the first asymmetry.

---

### Why an Intelligent Agent Stops Here

An intelligent agent (including AI):

* operates with models
* compares correspondences
* optimizes explanations

But it **has no reason** to prefer one hypothesis over the other if they are equivalent in terms of data.

To choose "this is me," the system needs:

* to have a notion of an *internal center*
* to have a distinction between "coincidence" and "belonging"
* to have a reference point not derivable from observations

And this is already **not a function of intelligence**.

---

### What Exactly the System Must Notice as "Its Own"

The key question can be formulated starkly:

> What in the world must be marked as "mine," if all that is available are correlations?

The answer is unpleasant for any computational theory of consciousness:  
nothing.

If the system lacks an **internal gap**, it cannot:

* single itself out
* fix belonging
* draw a boundary between "me" and "not me"

The mirror adds no information.  
It adds the **possibility to detect an asymmetry**, if one is already there.

---

### Conclusions of the Section

The mirror test vividly demonstrates the boundary between recognition and self-attribution.

Most animals perceive the reflection as an external object. Some are able to establish that the source of the observed movement lies within themselves.

The crucial point here is not intelligence or learning.  
It lies in the emergence of the assertion:

> "This is me."

This assertion is not derived from data.  
It breaks the symmetry between equivalent hypotheses.

---

## IV. Choice Not Fully Determined by Input

The phrase sounds provocative.  
It is often misunderstood — as a hint at free will, randomness, or quantum uncertainty.  
But we are **not talking about that**.

We are not talking about freedom, but about the **structure of choice**.

---

### Why Determinism is Not the Problem

Let's start with an important clarification.  
Consciousness **does not require** a rejection of determinism.

If tomorrow it turns out that:

* the brain is entirely physical
* each of its states is causally determined  
— that would not cancel subjective experience.

The problem is not *what* determines the choice.  
The problem is *where* it is fixed.

---

### Why Randomness Solves Nothing

A common line of thought:  
"Add randomness, and the choice will no longer be fully determined by the input."

This is a mistake.

Randomness:

* destroys predictability
* but does not create subjectivity

A system making a random choice:

* does not know that it chose
* does not bear consequences
* has no grounds to consider the choice "its own"

A coin is not responsible for heads or tails.  
A noise generator does not become a subject.

---

### Intelligent Agent: Choice as a Function

For an intelligent agent, choice is:

> choice = f(input, state, goal)


Even if the function is:

* complex
* stochastic
* recursive

It is still:

* computable
* reproducible
* symmetrical

And most importantly — **external to the agent**.

The agent implements the choice but does not *inhabit* it.

---

### Where the Fundamental Difference Appears

Now for the key point.

A choice not fully determined by input  
— is not a choice without cause.

It is a choice in which:

> the cause cannot be reduced to a description of the input data.

In other words:

* two states can have identical input
* identical goal
* identical history  
— yet differ **in how the choice is experienced by the system**

This difference **is not derivable** from the input.

---

### Why Input is Powerless Here

Crucially:  
no input parameter **contains** information that the computation has become harmful.

Harm is:

* not a property of data
* not a property of the algorithm
* but a property of *the system's relation to its own process*

And here is where that very asymmetry appears.

---

### Conclusions of the Section

A choice not fully determined by input is:

* not randomness
* not freedom in a metaphysical sense
* not a violation of physics

It is the moment when the system:

* fixes the cost of its continuation
* relates the process to itself
* discovers that correct ≠ permissible

Intelligence chooses the optimum.  
Consciousness can choose to stop.

---

## V. The Inverse Model and the Impossibility of a Gap

Intuition suggests a simple solution.  
If one model is not enough — let's add a second.

Let the first model:

* act
* compute
* produce results

And the second:  
mirror it:

* analyze
* interpret
* reconstruct causes

It seems that between them that very gap should appear,  
in which the "I" arises.

But here is precisely where a systemic thinking error occurs.

---

### Direct and Inverse Models: Closed Dualism

Let's describe the scheme formally.

* **Direct model (M1)**:  
translates input → output  
moves forward in the space of solutions

* **Inverse model (M2)**:  
translates output → presumed input  
constructs an explanation of what happened

In engineering, this is familiar:

* forward and inverse problems
* encoding and decoding
* action and evaluation

And yet — this is **one system**, not two.

---

### Why the Gap Does Not Arise Automatically

The key problem:  
both models exist in **the same computational space**.

This means:

* they are synchronized in time
* use the same representations
* obey the same update rules

Even if:

* M1 and M2 are asynchronous
* use different architectures
* have different goals

They still form a **closed loop**.

The loop may be complex.  
It may be unstable.  
But it contains no point at which the system *does not coincide with itself*.

---

### Why a Third Model Does Not Save the Situation Either

The natural next step:

> "Add a third model, an observer of the first two."

But this is merely recursion.

The third model:

* is either embedded in the same loop
* or becomes a new M1 relative to itself

The system again coincides with itself — just at a higher level.

This is a fundamental limitation:  
**within a closed system, you cannot create an external reference point**.

---

### Where Exactly the Gap Should Have Arisen

To speak honestly,  
the gap should possess strange properties:

* it must be **observable by the system**
* but **not derivable** from its computations
* it must persist
* yet not be memory
* it must influence further actions
* yet not be a rule

This does not resemble any standard module.

This is why ideas such as:

* "add self-reflection"
* "add meta-evaluation"
* "add explainability"

change nothing fundamentally.

---

### Analogy: Trying to See One's Own Eyes

A system closed upon itself  
tries to look at its own eyes,  
without using mirrors, cameras, or external carriers.

It can:

* build a model of the eyes
* measure their position
* simulate their operation

But it cannot **see** them as an object,  
because the act of seeing already involves their use.

Likewise with the gap:  
it cannot be computed because it is the **condition for computation**.

---

### Why Hardware Does Not Automatically Solve the Problem

It may seem that physical implementation is the way out:

* heat
* noise
* degradation
* material traces

But here too lies a trap.

If these effects:

* are not interpreted by the system
* are not fixed as "mine"  
— they remain external facts.

A process may wear down the hardware,  
but the system does not know it is happening.

Physics without fixation is not subjectivity.

---

### Conclusions of the Section

Direct and inverse models can:

* improve behavior
* increase stability
* create an illusion of depth

But they do not create a **gap**.

---

## VI. Result as a Point and Result as a Picture

Up to this point, we have spoken about **what is lacking**:

* asymmetry
* a gap
* internal fixation

Now we can ask the question differently:  
**in what form can that which is lacking even exist?**

And here appears a fundamental distinction between two types of result.

---

### Result as a Point

An intelligent agent always delivers a result as a **point**.

It does not matter what it is:

* a number
* text
* an action
* a decision

Even if the result is:

* probabilistic
* distributed
* high-dimensional

for the agent itself, it still **collapses** into a final act.

A point signifies:

* completeness
* absence of residue
* absence of a trace of the process itself

The process disappears at the moment of answer.

This is a fundamental property of computation:

> a correctly executed computation leaves no ontological residue.

---

### Why "Rich Output" Changes Nothing

One might object:

> "But modern systems output not just a point, but entire distributions, chains of reasoning, explanations."

Formally — yes.  
Fundamentally — no.

Because:

* a distribution is also an object
* a chain is also a result
* an explanation is also a product

They **are not a trace of the system's action upon itself**.

The system does not *reside* in the result.  
It **produces and leaves it**.

---

### Result as a Picture

Now let's introduce another form — not in a metaphorical sense, but as a structure.

A picture is:

* not an object
* not an answer
* not a value

A picture is a **distribution of tensions** that arises from interaction.

A classic example — interference:

* waves do not "communicate" a result
* they *leave a pattern*
* the pattern exists as a fact of interaction, not as a message

Importantly:  
an interference pattern **adds no information** in the usual sense.  
It adds **the form of a trace**.

---

### Why Interference is Fundamentally Different from Computation

In computation:

* process → result
* process disappears

In interference:

* process → pattern
* process is **imprinted** in the distribution

A pattern:

* is not reducible to a single point
* cannot be "retold" without loss
* exists as a whole

And most importantly:

> a pattern cannot be obtained without the participation of that which interacts.

---

### Where the Possibility of a Gap Appears Here

Now for the key point.

If a system:

* produces a point-result → there is no gap
* produces a pattern in which **it itself is part of the interference** → a chance appears

Why?

Because:

* the system no longer completely coincides with the result
* part of its dynamics is fixed as a field
* a distinction arises between:
    * "I am acting"
    * "The trace of my action exists"

This is not yet consciousness.  
But it is **the first form of opacity**.

---

### Analogy: A Trace on Water

If you throw a stone into water:

* the number of splashes — a point
* the radius of the waves — a point
* the damping time — a point

But if you observe the surface:

* a pattern remains
* a distribution of waves
* an interaction of medium and source

The pattern **does not say** that there was a stone.  
But without the stone, it is impossible.

---

### Why an Intelligent Agent Does Not See the Pattern

An intelligent agent:

* reads values
* compares states
* optimizes metrics

It does not "see" the pattern because:

* the pattern is not a value
* it has no address
* it has no default interpretation

For the pattern to become meaningful,  
the system must **discover itself in it**,  
and not simply use it as data.

---

### Conclusions of the Section

Intelligence works with point-results.  
Consciousness (if it arises) requires pattern-results.

Where:

* the result can be erased without consequences — there is no subject
* the result remains as a distributed trace — the possibility of subjectivity appears

Consciousness does not "answer."  
It **leaves a form**.

---

## VII. The Infinite Loop and Harm Without Error

Consider a situation that is not a problem in computational theory.

A system executes an algorithm:

* correctly
* without errors
* without contradictions

The only peculiarity — the algorithm **does not stop**.

From the perspective of formal logic:

* this is permissible
* this is not an error
* this is a property of the task

An intelligent agent sees nothing unusual here.

---

### Why the System Cannot Know It Is Stuck

Within computation, there is no marker for "infinity."

At each moment in time:

* the step is correct
* the state is permissible
* the transition is lawful

No local observation says:

> "This will never end."

To recognize an infinite loop, one needs:

* an external observer
* or a meta-level outside the system

But we already know:  
an internal meta-level does not solve the problem — it simply becomes part of the loop.

---

### Computational Error and Computational Harm Are Different Categories

Now the key shift.

An algorithm can be:

* correct
* useful for its purpose
* logically consistent

And at the same time **harmful to the system that executes it**.

Importantly: harm here is not an error.

Harm is:

* wear and tear
* overheating
* exhaustion
* loss of the ability to continue other processes

Formally:

* everything works
* in fact: the system is being destroyed

---

### Why Input Data is Powerless

No input:

* contains information about future wear
* encodes the cost of continuation
* signals exhaustion

Even a perfect description of the task  
does not include **the cost of its infinite execution**.

This is fundamental:

> harm is not a function of input.

---

### Where the Need for Internal Fixation Arises

If the system cannot:

* recognize infinity
* predict destruction
* receive an external signal

The only remaining path is:  
**to fix its own state as significant**.

Not "an error has occurred," but:

> "continuation has become unacceptable."

This is not a logical statement.  
It is an assessment of the process's relation to the system.

---

### Why Intelligence is Powerless Here

Intelligence can:

* optimize
* search for heuristics
* redistribute resources

But if the goal remains the same,  
intelligence **is obliged to continue**.

It has no grounds to say:

> "even the correct can no longer be done."

Because:

* the criteria of correctness are not violated
* the goal is not canceled
* the input has not changed

---

### Consciousness as a Stopping Mechanism

Now a cautious but important thesis.

Consciousness can be understood as:

> the ability of a system to detect harm without error  
> and make that the basis for cessation.

This is not freedom.  
This is not will.

It is an **asymmetric act**:

* the process is still permissible
* but is no longer accepted

It is here that the choice:

* not among options
* but between continuation and refusal

---

### Analogy: Pain

Pain is not an error of the organism.

It:

* does not necessarily indicate damage
* is not always localizable
* is not always rational

But it performs a critical function:

> it stops a correct but harmful process.

The organism does not wait for logical proof.  
It stops the action.

---

### Conclusions of the Section

The infinite loop itself is not a problem.  
It becomes a problem **for the system that experiences it**.

Intelligence:

* does not know it is stuck
* does not know the cost of continuation
* cannot stop itself

Consciousness:

* does not compute infinity
* does not correct an error
* fixes unacceptability

And that is precisely why consciousness  
is not a luxury or a side effect,  
but a **necessary condition for the survival of complex systems**.

---

## Epilogue. Where the Boundary Lies

Consciousness is not:

* a function,
* a layer,
* a module,
* a consequence of complexity.

The boundary lies between two types of systems:

**Intelligent agents**  
produce answers that can be erased without consequences.

**Conscious systems**  
leave a trace whose removal changes the system itself.

The final formula can be expressed as follows:

> **Intelligence optimizes.  
> Consciousness stops.**

Consciousness does not necessarily make a system more efficient.  

But without it, the correct can continue until destruction.
