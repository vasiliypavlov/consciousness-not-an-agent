## Note on this version

This English text is an AI-assisted draft based on the original Russian article
"Why Consciousness Is Not Reducible to an Intelligent Agent".

https://github.com/vasiliypavlov/consciousness-not-an-agent/blob/main/text/consciousness-not-an-agent-ru.md

It is provided for orientation and discussion purposes only.
The author considers it conceptually simplified and not equivalent
to the original text.

A revised English version may be prepared in the future.

# Consciousness Is Not an Intelligent Agent

## Prologue

Contemporary intelligent systems demonstrate increasingly complex behavior:  
they learn, explain their actions, adapt to feedback, and optimize outcomes.  
This naturally leads to the assumption that consciousness is merely a matter of scale.

This text argues the opposite.

Consciousness is not a continuation of intelligence,  
even in the limit of infinite complexity.

The question here is not *how* to build consciousness,  
but *why all known computational approaches fail to produce it*.

---

## I. A Model That Is Not Equal to Itself

Any intelligent system changes over time.  
It learns, accumulates statistics, and updates its internal representations.

At first glance, this temporal asymmetry may appear sufficient:  
the model “before” is not equal to the model “after”.

Yet this difference alone does not generate subjectivity.

From the system’s own perspective,  
there are only state transitions governed by rules.

The system may change,  
but it does not register the fact of its own change as meaningful to itself.

Learning remains an internal transaction,  
not an experienced transformation.

---

## II. Asymmetry as a Necessary Condition

Most attempts to reduce consciousness to computation rely on an implicit belief:  
sufficient complexity will eventually produce subjectivity.

This belief is mistaken.

The missing ingredient is not complexity, but **asymmetry**.

Computation is fundamentally symmetric:
- input maps to output
- state maps to state
- rules apply uniformly

Even stochastic, adaptive, self-modifying systems  
remain invariant with respect to their own description.

Asymmetry here does not mean randomness.  
It means **irreversibility that cannot be derived from the algorithm itself**.

In engineering terms:  
a state whose removal would change the system, not merely its data.

---

## III. The Mirror and the Moment of Self-Recognition

The mirror test illustrates the distinction between recognition and self-reference.

Most animals perceive the reflection as an external object.  
A few recognize it as themselves.

This moment does not arise from additional data or processing power.

It introduces a rupture:

> “This is me.”

That statement is not inferred.  
It breaks the symmetry between equivalent hypotheses.

---

## IV. A Choice Not Fully Determined by the Input

A choice not fully determined by input  
is not a choice without cause.

It is a choice where:
- the cause cannot be reduced to the input description
- identical inputs may lead to different internal valuations
- correctness is no longer equivalent to acceptability

Randomness does not help.  
Noise destroys predictability but preserves symmetry.

What matters is the emergence of **cost**.

---

## V. The Reverse Model and the Impossibility of a Gap

A common intuition suggests a solution:  
if one model is insufficient, add another.

Let one model act,  
and another analyze, explain, or reconstruct the action.

Yet forward and reverse models occupy the same computational space.
They share representations, timing, and update rules.

Even a third observer model only deepens the recursion.

Within a closed system,  
no external reference point can emerge.

The desired gap cannot be computed  
because it is the condition of computation itself.

---

## VI. Result as a Point and Result as a Pattern

An intelligent agent always produces results as points:
- an answer
- an action
- a decision

Even probabilistic or distributed outputs collapse into a final act.

Computation leaves no ontological residue.

By contrast, a pattern—such as an interference pattern—
is not a message but a trace of interaction.

It cannot be reduced to a single value  
without losing what it is.

A system that leaves such a trace  
is no longer fully identical with its output.

This is not yet consciousness,  
but it is the first form of opacity.

---

## VII. Infinite Loops and Harm Without Error

A correct algorithm may run forever.

Formally, this is not a problem.

Each step is valid.  
Each transition is legal.

Yet the system executing it may degrade, exhaust resources, or fail.

Harm is not an error.

It is not a property of data or algorithms,  
but of the relationship between a process and the system that sustains it.

No input encodes the cost of infinite continuation.

---

## Consciousness as a Stopping Mechanism

At this point, a cautious claim becomes possible.

Consciousness can be understood as  
the ability to register harm without error  
and to make this registration decisive.

Not as freedom.  
Not as metaphysical will.

But as an asymmetric act:

- the process remains correct
- yet continuation is rejected

This is not a choice among options,  
but a refusal of continuation itself.

---

## Epilogue. Where the Boundary Lies

Consciousness is not:
- a function
- a module
- a layer
- a byproduct of complexity

The boundary lies between two kinds of systems.

**Intelligent agents**  
produce results that can be erased without consequence.

**Conscious systems**  
leave traces whose removal alters the system itself.

The distinction can be stated plainly:

> **Intelligence optimizes.  
> Consciousness stops.**

Consciousness does not necessarily make systems smarter.  
But without it, correctness may continue until destruction.

